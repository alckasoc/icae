{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf35aa3-df45-42e7-85eb-1d52b4486109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from finetune_gsm8kcot_ae import ModelArguments, TrainingArguments, DataArguments\n",
    "\n",
    "model_args, training_args, data_args = ModelArguments(), TrainingArguments(output_dir=\"./output\"), DataArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b430648d-69fa-4fd1-bd2b-b25e70b28b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ankner/gsm8k-CoT\")\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fd4212-60d7-4b79-aba2-51cd6f84a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})\n",
    "eval_dataset = eval_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df40f0aa-c0b3-409a-828a-47e9bd2d40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import pretrain_tokenize_function\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments()\n",
    "training_args = TrainingArguments(output_dir=\"./output\")\n",
    "data_args = DataArguments()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=model_args.lora_alpha,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e514bfdb-05e5-4d1a-bf8e-24eb0fcd7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13639680 || all params: 2485278720 || trainable%: 0.5488189268365039\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9264a71-c658-4c54-8dcb-33e81c7a76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune_gsm8kcot_ae import preprocess_function\n",
    "\n",
    "memory_size = training_args.fixed_mem_size\n",
    "MEM_TOKENS = list(range(model.vocab_size, model.vocab_size + memory_size))\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function)\n",
    "eval_dataset = eval_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a8d4e6-bac6-4cb9-8467-cac9497d3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select([0])\n",
    "eval_dataset = eval_dataset.select([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d364105a-066c-4972-9f44-37cd6ff863ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97fe6712de84b64afaee59209aa59b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's say the first ship had x people.\\nThe second ship had 2x people.\\nThe third ship had 4x people.\\nThe total number of people consumed is 847, which means:\\nx + 2x + 4x = 847\\n7x = 847\\nx = 121\"]\n",
      "{'input_ids': [[128000, 10267, 596, 2019, 279, 1176, 8448, 1047, 865, 1274, 627, 791, 2132, 8448, 1047, 220, 17, 87, 1274, 627, 791, 4948, 8448, 1047, 220, 19, 87, 1274, 627, 791, 2860, 1396, 315, 1274, 27073, 374, 220, 25125, 11, 902, 3445, 512, 87, 489, 220, 17, 87, 489, 220, 19, 87, 284, 220, 25125, 198, 22, 87, 284, 220, 25125, 198, 87, 284, 220, 7994]]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12072e692f14d3997bc7c59dfab57d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A robe needs 2 bolts of blue fiber.\\nThe amount of white fiber needed is half of the blue fiber.\\nHalf of 2 bolts is 1 bolt of white fiber.\\nThe total bolts needed is the sum of blue and white fiber.\\n2 bolts plus 1 bolt equals 3 bolts.']\n",
      "{'input_ids': [[128000, 32, 63719, 3966, 220, 17, 49939, 315, 6437, 24722, 627, 791, 3392, 315, 4251, 24722, 4460, 374, 4376, 315, 279, 6437, 24722, 627, 43727, 315, 220, 17, 49939, 374, 220, 16, 32942, 315, 4251, 24722, 627, 791, 2860, 49939, 4460, 374, 279, 2694, 315, 6437, 323, 4251, 24722, 627, 17, 49939, 5636, 220, 16, 32942, 17239, 220, 18, 49939, 13]]}\n"
     ]
    }
   ],
   "source": [
    "from training_utils import pretrain_tokenize_function\n",
    "\n",
    "train_dataset = train_dataset.map(pretrain_tokenize_function, batched=True, batch_size=1, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS, \"lm_ratio\": training_args.lm_ratio})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "896eee3b-85a0-4162-a433-7950ff61d012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'answer', 'response', 'text', 'reasoning_trace', 'input_ids', 'prompt_answer_ids', 'labels'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "260aa275-8f5a-4172-a5ad-69b83ea1698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's say the first ship had x people.\\nThe second ship had 2x people.\\nThe third ship had 4x people.\\nThe total number of people consumed is 847, which means:\\nx + 2x + 4x = 847\\n7x = 847\\nx = 121\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['reasoning_trace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39b88fa4-5acb-4cce-8c3c-d83a3cc2f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's say the first ship had x people.\n",
      "The second ship had 2x people.\n",
      "The third ship had 4x people.\n",
      "The total number of people consumed is 847, which means:\n",
      "x + 2x + 4x = 847\n",
      "7x = 847\n",
      "x = 121#\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoded_text = model.tokenizer.decode(train_dataset[0]['labels'][3:], skip_special_tokens=True)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f302b8fe-2088-4b2d-ab9e-cb4da0e279ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 20 05:40:03 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:06:00.0 Off |                  Off |\n",
      "| 30%   34C    P8             22W /  300W |     271MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3291      C   /usr/bin/python3                              264MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05de19aa-2b87-4856-9634-b7126535d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13639680 || all params: 2485278720 || trainable%: 0.5488189268365039\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bfa597b-c046-44e5-87da-6ccb074531b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3291/1249772467.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"output/model_weights.pth\"), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load(\"output/model_weights.pth\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "751a1e76-278d-4528-9f8c-e1f2b86eb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_inference(model, lines):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    print(\"Running inference\")\n",
    "    with torch.no_grad():\n",
    "        for line in tqdm(lines):\n",
    "            print(\"=========================== START ============================\")\n",
    "            print(\"Current line: \", line)\n",
    "            # Tokenize input text\n",
    "            tokenized_text = model.tokenizer(line, truncation=True,\n",
    "                                          max_length=5120, padding=False,\n",
    "                                          return_attention_mask=False)\n",
    "            # Generate compressed outputs\n",
    "            input_ids = torch.LongTensor([tokenized_text['input_ids']]).to(device)\n",
    "            print(\"input_ids shape: \", input_ids.size())\n",
    "            memory_slots = model._compress(input_ids)\n",
    "            print(\"memory_slots shape: \", memory_slots.size())\n",
    "            \n",
    "            # prompt_output = model.tokenizer(data['prompt'], add_special_tokens=False, padding=False)\n",
    "            prompt_ids = torch.LongTensor([[model.ae_token_id]]).to(device)\n",
    "            print(\"prompt_ids shape: \", prompt_ids.size())\n",
    "\n",
    "            prompt_answer_embs = model.tokens_to_embeddings(prompt_ids)\n",
    "            print(\"prompt_answer_embs shape: \", prompt_answer_embs.size())\n",
    "\n",
    "            memory_slots = memory_slots.to(prompt_answer_embs)\n",
    "                        \n",
    "            # Concatenate and clone input embeddings\n",
    "            decoder_input_embeddings = torch.cat((memory_slots.unsqueeze(0), prompt_answer_embs), dim=1)\n",
    "            print(\"decoder_input_embeddings shape: \", decoder_input_embeddings.size())\n",
    "\n",
    "            output = decoder_input_embeddings.clone()\n",
    "            print(\"output shape: \", output.size())\n",
    "\n",
    "            generate_text = []\n",
    "            past_key_values = None\n",
    "\n",
    "            # Generate text output\n",
    "            for i in range(512):\n",
    "                with model.icae.disable_adapter():   # no independent decoder; use self.icae\n",
    "                    out = model.icae(inputs_embeds=output, past_key_values=past_key_values, use_cache=True)\n",
    "                logit = out.logits[:, -1, :model.vocab_size-1]\n",
    "                past_key_values = out.past_key_values\n",
    "\n",
    "                next_token_id = torch.argmax(logit, dim=-1)\n",
    "                # print(next_token_id)\n",
    "                \n",
    "                if next_token_id.item() == 2:   # eos\n",
    "                    break\n",
    "\n",
    "                output = model.icae.get_base_model().model.embed_tokens(next_token_id).unsqueeze(1).to(device)\n",
    "                generate_text.append(next_token_id.item())\n",
    "\n",
    "            generated_text = model.tokenizer.decode(generate_text)\n",
    "            outputs.append(generated_text)\n",
    "\n",
    "            print(\"=========================== END ============================\")\n",
    "\n",
    "    return outputs, memory_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ade51989-9ed0-482d-aaea-e005a43ce2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0fe55a7-838a-48d2-8ca3-a86fcb44decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== START ============================\n",
      "Current line:  Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\n",
      "input_ids shape:  torch.Size([1, 105])\n",
      "memory_slots shape:  torch.Size([1, 2048])\n",
      "prompt_ids shape:  torch.Size([1, 1])\n",
      "prompt_answer_embs shape:  torch.Size([1, 1, 2048])\n",
      "decoder_input_embeddings shape:  torch.Size([1, 2, 2048])\n",
      "output shape:  torch.Size([1, 2, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:13<00:00, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== END ============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\"\n",
    "]\n",
    "\n",
    "device = \"cuda\"\n",
    "outputs, memory_slots = run_inference(model, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e8d6327-58c9-481d-b88b-bf6827b248ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_slots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03e35807-ea01-45fa-a5ae-ad0bdcc0d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3633, -1.6016,  0.3457,  ..., -0.5391,  1.1250, -0.9766]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "914651da-408d-4530-bb8d-4ecc15db7afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå **Memory Slot Debugging** üìå\n",
      "‚û°Ô∏è Mean Value: 0.023682\n",
      "‚û°Ô∏è Standard Deviation: 2.296875\n",
      "‚û°Ô∏è Min Value: -18.125000\n",
      "‚û°Ô∏è Max Value: 11.187500\n",
      "‚û°Ô∏è Percentage of Near-Zero Values: 0.00%\n",
      "‚úÖ Memory embeddings contain significant nonzero values.\n",
      "\n",
      "üõ† Sample Memory Slot Values:\n",
      "tensor([-0.3633, -1.6016,  0.3457,  1.7578, -0.7969,  4.3125,  1.3984,  1.4297,\n",
      "         1.7969,  3.3438], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_memory_slots(memory_slots):\n",
    "    \"\"\"\n",
    "    Checks whether the memory slot embeddings contain meaningful values \n",
    "    or are close to zero.\n",
    "\n",
    "    Args:\n",
    "        memory_slots (torch.Tensor): The compressed memory representation from ICAE.\n",
    "    \"\"\"\n",
    "    # Move to CPU for easier analysis\n",
    "    memory_slots = memory_slots.detach().cpu()\n",
    "\n",
    "    # Compute statistics\n",
    "    mean_value = memory_slots.mean().item()\n",
    "    std_value = memory_slots.std().item()\n",
    "    min_value = memory_slots.min().item()\n",
    "    max_value = memory_slots.max().item()\n",
    "\n",
    "    print(\"\\nüìå **Memory Slot Debugging** üìå\")\n",
    "    print(f\"‚û°Ô∏è Mean Value: {mean_value:.6f}\")\n",
    "    print(f\"‚û°Ô∏è Standard Deviation: {std_value:.6f}\")\n",
    "    print(f\"‚û°Ô∏è Min Value: {min_value:.6f}\")\n",
    "    print(f\"‚û°Ô∏è Max Value: {max_value:.6f}\")\n",
    "\n",
    "    # Check if most values are near zero\n",
    "    zero_threshold = 1e-5\n",
    "    near_zero_ratio = (torch.abs(memory_slots) < zero_threshold).float().mean().item()\n",
    "\n",
    "    print(f\"‚û°Ô∏è Percentage of Near-Zero Values: {near_zero_ratio * 100:.2f}%\")\n",
    "    \n",
    "    if near_zero_ratio > 0.90:\n",
    "        print(\"‚ö†Ô∏è WARNING: Memory embeddings are mostly zero! Encoder may not be learning meaningful compression.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Memory embeddings contain significant nonzero values.\")\n",
    "\n",
    "    # Print a small sample of memory slot values\n",
    "    print(\"\\nüõ† Sample Memory Slot Values:\")\n",
    "    print(memory_slots[0, :10])  # Print first 10 values of the first memory slot\n",
    "\n",
    "check_memory_slots(memory_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1940f15-291a-41fb-81f5-25c5ad5d933c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aa13ca7a5544bd9f5bf9e18ebca147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b59e156e8c43b69f776d103ec29f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully...\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"ankner/gsm8k-CoT\")\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"test\"]\n",
    "\n",
    "train_dataset = train_dataset.map(lambda example: {**example, \"text\": extract_reasoning_trace(example['response'])}).shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.map(lambda example: {**example, \"text\": extract_reasoning_trace(example['response'])}).shuffle(seed=42)\n",
    "print(\"Dataset loaded successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba87d2d8-4486-4c52-acb1-fb7350ddfd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\",\n",
       " 'answer': '40',\n",
       " 'response': 'Each adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 √ó (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 √ó (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 √ó (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.\\n\\nTherefore, the final answer is 40.',\n",
       " 'text': 'Each adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 √ó (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 √ó (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 √ó (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5edf8b-4f76-4ab2-97b2-cf6c0eaf0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CosineEmbeddingLoss\n",
    "\n",
    "# Mean embedding of input\n",
    "input_mean_embedding = segment_input_embedding.mean(dim=1).detach()\n",
    "\n",
    "# Contrastive loss to ensure memory stores structured meaning\n",
    "contrastive_loss_fct = CosineEmbeddingLoss(margin=0.5)\n",
    "contrastive_target = torch.ones(memory_slots.shape[0]).to(memory_slots.device)\n",
    "contrastive_loss = contrastive_loss_fct(memory_slots, input_mean_embedding, contrastive_target)\n",
    "\n",
    "# Add contrastive loss to training loss\n",
    "loss += 0.1 * contrastive_loss  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
