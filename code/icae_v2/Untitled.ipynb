{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf56cd75-36f2-483e-8379-99ce0f71e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ace28b-2312-40d8-a9b3-bdad93c5da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 09:34:12.411649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739612052.441668    2967 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739612052.450721    2967 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE, ModelArguments, DataArguments, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594b7d15-508a-4a35-9f98-a251f377ea4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ee7c66-b657-44a0-93de-5e420aa53fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HfArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8e0cc5-4f46-4454-b580-948429aad7b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--lora_r LORA_R] [--lora_dropout LORA_DROPOUT]\n",
      "                             [--train [TRAIN]] [--no_train]\n",
      "                             [--data_path DATA_PATH]\n",
      "                             [--debug_data [DEBUG_DATA]] --output_dir\n",
      "                             OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--eval_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch,best}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim OPTIM] [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo HUB_PRIVATE_REPO]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]\n",
      "                             [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "                             [--no_eval_do_concat_batches]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--evaluation_strategy {no,steps,epoch}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--dispatch_batches DISPATCH_BATCHES]\n",
      "                             [--split_batches SPLIT_BATCHES]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                             [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                             [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "                             [--eval_on_start [EVAL_ON_START]]\n",
      "                             [--use_liger_kernel [USE_LIGER_KERNEL]]\n",
      "                             [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "                             [--average_tokens_across_devices [AVERAGE_TOKENS_ACROSS_DEVICES]]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--model_max_length MODEL_MAX_LENGTH]\n",
      "                             [--fixed_mem_size FIXED_MEM_SIZE]\n",
      "                             [--mean_compression_rate MEAN_COMPRESSION_RATE]\n",
      "                             [--min_tokens_for_lm MIN_TOKENS_FOR_LM]\n",
      "                             [--leave_tokens_for_lm LEAVE_TOKENS_FOR_LM]\n",
      "                             [--lm_ratio LM_RATIO]\n",
      "                             [--add_special_token_for_lm [ADD_SPECIAL_TOKEN_FOR_LM]]\n",
      "                             [--restore_from RESTORE_FROM]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_dir/--output-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04874c3-a0cb-4ebd-a42d-5adee1817451",
   "metadata": {},
   "source": [
    "# OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf35aa3-df45-42e7-85eb-1d52b4486109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e15f49-d5a1-441c-ad19-5d3ee07b3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(default=\"mistralai/Mistral-7B-v0.1\")\n",
    "    lora_r: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"lora rank\"}\n",
    "    )\n",
    "    lora_dropout: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\"help\": \"lora dropout\"}\n",
    "    )\n",
    "    train: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"if true, the model ckpt will be initialized for training; else, it's for inference\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    debug_data: bool = field(default=False, metadata={\"help\": \"Enable debug dataset to quickly verify the training process\"})\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=28000,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    fixed_mem_size: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"Enalbing the fixed mem size.\"},\n",
    "    )\n",
    "    mean_compression_rate: int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Mean compression rate; default=4\"},\n",
    "    )\n",
    "    min_tokens_for_lm: int = field(\n",
    "        default=64,\n",
    "        metadata={\"help\": \"Minimum tokens for lm objective learning\"},\n",
    "    )\n",
    "    leave_tokens_for_lm: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Leave some tokens without loss for lm objective\"},\n",
    "    )\n",
    "    lm_ratio: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Ratio for LM training.\"},\n",
    "    )\n",
    "    add_special_token_for_lm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add a special token for the prompt of language modeling; default: False\"},\n",
    "    )\n",
    "    restore_from: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\": \"The checkpoint that should be restored from for fine-tuning\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a670ffb-e8cc-4776-a256-11963b4283c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 00:33:45.903008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739752425.916470     835 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739752425.920499     835 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 00:33:45.935419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='mistralai/Mistral-7B-v0.1', lora_r=128, lora_dropout=0.05, train=True),\n",
       " TrainingArguments(output_dir='./output', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Feb17_00-33-43_80f84540ae5d', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, eval_use_gather_object=False, average_tokens_across_devices=False, cache_dir=None, model_max_length=28000, fixed_mem_size=128, mean_compression_rate=4, min_tokens_for_lm=64, leave_tokens_for_lm=8, lm_ratio=0.0, add_special_token_for_lm=False, restore_from=''),\n",
       " DataArguments(data_path=None, debug_data=False))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelArguments(), TrainingArguments(output_dir=\"./output\"), DataArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b430648d-69fa-4fd1-bd2b-b25e70b28b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc34a4c9e904c558992c3f106645e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/448 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b63ee274004d1abe2446b99991a206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98457074934a4e1fb3da8f4341f5556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/440k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cab4ab0c9f4c74941b45a4a9357c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77791e5583b94864aa680153885ffbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ankner/gsm8k-CoT\")\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fd4212-60d7-4b79-aba2-51cd6f84a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4794d9279b04466c9a5b4ff7967a0fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dab1d30d3024984bfe9262310b89b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})\n",
    "eval_dataset = eval_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df40f0aa-c0b3-409a-828a-47e9bd2d40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import pretrain_tokenize_function\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments()\n",
    "training_args = TrainingArguments(output_dir=\"./output\")\n",
    "data_args = DataArguments()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92291d9f-5671-44df-928d-9a0fadc9c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path = \"meta-llama/Llama-3.2-1B\"\n",
    "training_args.bf16 = True\n",
    "training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}  # manually add this argument in the code\n",
    "training_args.lm_ratio = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acfdd6f1-845a-4b60-8027-8e4c4f2678cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b53d0d6-9f55-448e-aa21-375bf6cfe697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# import torch\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# icae = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", resume_download=True)\n",
    "# icae = icae.to(\"cuda\")\n",
    "# icae.config.vocab_size, icae.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e514bfdb-05e5-4d1a-bf8e-24eb0fcd7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13899776 || all params: 2485798912 || trainable%: 0.5591673539198975\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e725d9e-97a0-476c-b39f-14facd14270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13899776 || all params: 2485798912 || trainable%: 0.5591673539198975\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Initialize model and send it to CUDA device\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "859979b9-df52-4d8b-8e1c-6fdb428c3292",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ICAE:\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ICAE:\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 128]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for icae.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([512, 512])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50a0f9e-183f-43f1-8167-ba7d6eca42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'response': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'prompt_answer_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2489cc41-bbdf-47e7-bd3c-a6e7f2285882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 17 00:35:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:03:00.0 Off |                  Off |\n",
      "| 30%   34C    P2             53W /  300W |     435MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e2cb3d-afad-48b6-9fe8-8787e5ec0c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0e000e7c6b4a239d53534cbc438f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869df819e4b0439ba1d91c1d5ca029e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory_size = 128\n",
    "MEM_TOKENS = list(range(model.vocab_size, model.vocab_size + memory_size))\n",
    "\n",
    "train_dataset = train_dataset.map(pretrain_tokenize_function, batched=True, batch_size=1, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS, \"lm_ratio\": training_args.lm_ratio})\n",
    "eval_dataset = eval_dataset.map(pretrain_tokenize_function, batched=True, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS})   # don't add lm in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd5d82e4-e3c9-44fc-9e13-40a578e7da9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 128000,\n",
       " 32,\n",
       " 5655,\n",
       " 7962,\n",
       " 64,\n",
       " 18118,\n",
       " 38268,\n",
       " 505,\n",
       " 279,\n",
       " 21160,\n",
       " 3131,\n",
       " 1475,\n",
       " 7895,\n",
       " 1667,\n",
       " 311,\n",
       " 53268,\n",
       " 389,\n",
       " 264,\n",
       " 8448,\n",
       " 323,\n",
       " 274,\n",
       " 349,\n",
       " 1202,\n",
       " 34906,\n",
       " 13,\n",
       " 6193,\n",
       " 2380,\n",
       " 7895,\n",
       " 1667,\n",
       " 11,\n",
       " 433,\n",
       " 706,\n",
       " 27073,\n",
       " 220,\n",
       " 25125,\n",
       " 1274,\n",
       " 13,\n",
       " 74632,\n",
       " 617,\n",
       " 1027,\n",
       " 5918,\n",
       " 8294,\n",
       " 927,\n",
       " 892,\n",
       " 11,\n",
       " 779,\n",
       " 1855,\n",
       " 502,\n",
       " 8448,\n",
       " 706,\n",
       " 11157,\n",
       " 439,\n",
       " 1690,\n",
       " 1274,\n",
       " 439,\n",
       " 279,\n",
       " 1566,\n",
       " 8448,\n",
       " 13,\n",
       " 2650,\n",
       " 1690,\n",
       " 1274,\n",
       " 1051,\n",
       " 389,\n",
       " 279,\n",
       " 8448,\n",
       " 279,\n",
       " 18118,\n",
       " 30912,\n",
       " 304,\n",
       " 279,\n",
       " 1176,\n",
       " 7895,\n",
       " 1667,\n",
       " 5380,\n",
       " 10267,\n",
       " 596,\n",
       " 2019,\n",
       " 279,\n",
       " 1176,\n",
       " 8448,\n",
       " 1047,\n",
       " 865,\n",
       " 1274,\n",
       " 627,\n",
       " 791,\n",
       " 2132,\n",
       " 8448,\n",
       " 1047,\n",
       " 220,\n",
       " 17,\n",
       " 87,\n",
       " 1274,\n",
       " 627,\n",
       " 791,\n",
       " 4948,\n",
       " 8448,\n",
       " 1047,\n",
       " 220,\n",
       " 19,\n",
       " 87,\n",
       " 1274,\n",
       " 627,\n",
       " 791,\n",
       " 2860,\n",
       " 1396,\n",
       " 315,\n",
       " 1274,\n",
       " 27073,\n",
       " 374,\n",
       " 220,\n",
       " 25125,\n",
       " 11,\n",
       " 902,\n",
       " 3445,\n",
       " 512,\n",
       " 87,\n",
       " 489,\n",
       " 220,\n",
       " 17,\n",
       " 87,\n",
       " 489,\n",
       " 220,\n",
       " 19,\n",
       " 87,\n",
       " 284,\n",
       " 220,\n",
       " 25125,\n",
       " 198,\n",
       " 22,\n",
       " 87,\n",
       " 284,\n",
       " 220,\n",
       " 25125,\n",
       " 198,\n",
       " 87,\n",
       " 284,\n",
       " 220,\n",
       " 7994,\n",
       " 198,\n",
       " 55915,\n",
       " 11,\n",
       " 279,\n",
       " 1620,\n",
       " 4320,\n",
       " 374,\n",
       " 220,\n",
       " 7994,\n",
       " 13,\n",
       " 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6322cc21-7576-473b-a657-449467356db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 17 02:36:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:03:00.0 Off |                  Off |\n",
      "| 30%   36C    P8             19W /  300W |     435MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019ba5e-87c6-4e03-bb86-3ba7aaaa177b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
