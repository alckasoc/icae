{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf56cd75-36f2-483e-8379-99ce0f71e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ace28b-2312-40d8-a9b3-bdad93c5da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 09:34:12.411649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739612052.441668    2967 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739612052.450721    2967 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE, ModelArguments, DataArguments, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594b7d15-508a-4a35-9f98-a251f377ea4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ee7c66-b657-44a0-93de-5e420aa53fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HfArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8e0cc5-4f46-4454-b580-948429aad7b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                             [--lora_r LORA_R] [--lora_dropout LORA_DROPOUT]\n",
      "                             [--train [TRAIN]] [--no_train]\n",
      "                             [--data_path DATA_PATH]\n",
      "                             [--debug_data [DEBUG_DATA]] --output_dir\n",
      "                             OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--eval_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--eval_delay EVAL_DELAY]\n",
      "                             [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]\n",
      "                             [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                             [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                             [--no_log_on_each_node]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                             [--no_logging_nan_inf_filter]\n",
      "                             [--save_strategy {no,steps,epoch,best}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                             [--no_save_safetensors]\n",
      "                             [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                             [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                             [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "                             [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                             [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                             [--data_seed DATA_SEED]\n",
      "                             [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                             [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                             [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug DEBUG [DEBUG ...]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--fsdp FSDP]\n",
      "                             [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                             [--fsdp_config FSDP_CONFIG]\n",
      "                             [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                             [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--optim OPTIM] [--optim_args OPTIM_ARGS]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                             [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--no_skip_memory_metrics]\n",
      "                             [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                             [--push_to_hub [PUSH_TO_HUB]]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_private_repo HUB_PRIVATE_REPO]\n",
      "                             [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                             [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                             [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                             [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                             [--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]\n",
      "                             [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "                             [--no_eval_do_concat_batches]\n",
      "                             [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                             [--evaluation_strategy {no,steps,epoch}]\n",
      "                             [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                             [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                             [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "                             [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                             [--full_determinism [FULL_DETERMINISM]]\n",
      "                             [--torchdynamo TORCHDYNAMO]\n",
      "                             [--ray_scope RAY_SCOPE]\n",
      "                             [--ddp_timeout DDP_TIMEOUT]\n",
      "                             [--torch_compile [TORCH_COMPILE]]\n",
      "                             [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                             [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                             [--dispatch_batches DISPATCH_BATCHES]\n",
      "                             [--split_batches SPLIT_BATCHES]\n",
      "                             [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                             [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                             [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                             [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                             [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "                             [--eval_on_start [EVAL_ON_START]]\n",
      "                             [--use_liger_kernel [USE_LIGER_KERNEL]]\n",
      "                             [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "                             [--average_tokens_across_devices [AVERAGE_TOKENS_ACROSS_DEVICES]]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--model_max_length MODEL_MAX_LENGTH]\n",
      "                             [--fixed_mem_size FIXED_MEM_SIZE]\n",
      "                             [--mean_compression_rate MEAN_COMPRESSION_RATE]\n",
      "                             [--min_tokens_for_lm MIN_TOKENS_FOR_LM]\n",
      "                             [--leave_tokens_for_lm LEAVE_TOKENS_FOR_LM]\n",
      "                             [--lm_ratio LM_RATIO]\n",
      "                             [--add_special_token_for_lm [ADD_SPECIAL_TOKEN_FOR_LM]]\n",
      "                             [--restore_from RESTORE_FROM]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --output_dir/--output-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04874c3-a0cb-4ebd-a42d-5adee1817451",
   "metadata": {},
   "source": [
    "# OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf35aa3-df45-42e7-85eb-1d52b4486109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e15f49-d5a1-441c-ad19-5d3ee07b3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(default=\"mistralai/Mistral-7B-v0.1\")\n",
    "    lora_r: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"lora rank\"}\n",
    "    )\n",
    "    lora_dropout: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\"help\": \"lora dropout\"}\n",
    "    )\n",
    "    train: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"if true, the model ckpt will be initialized for training; else, it's for inference\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    debug_data: bool = field(default=False, metadata={\"help\": \"Enable debug dataset to quickly verify the training process\"})\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=28000,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    fixed_mem_size: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"Enalbing the fixed mem size.\"},\n",
    "    )\n",
    "    mean_compression_rate: int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Mean compression rate; default=4\"},\n",
    "    )\n",
    "    min_tokens_for_lm: int = field(\n",
    "        default=64,\n",
    "        metadata={\"help\": \"Minimum tokens for lm objective learning\"},\n",
    "    )\n",
    "    leave_tokens_for_lm: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Leave some tokens without loss for lm objective\"},\n",
    "    )\n",
    "    lm_ratio: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Ratio for LM training.\"},\n",
    "    )\n",
    "    add_special_token_for_lm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add a special token for the prompt of language modeling; default: False\"},\n",
    "    )\n",
    "    restore_from: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\": \"The checkpoint that should be restored from for fine-tuning\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a670ffb-e8cc-4776-a256-11963b4283c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 05:22:59.232518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739856179.254742    2489 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739856179.261374    2489 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='mistralai/Mistral-7B-v0.1', lora_r=128, lora_dropout=0.05, train=True),\n",
       " TrainingArguments(output_dir='./output', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Feb18_05-22-57_104-171-202-244', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, eval_use_gather_object=False, average_tokens_across_devices=False, cache_dir=None, model_max_length=28000, fixed_mem_size=128, mean_compression_rate=4, min_tokens_for_lm=64, leave_tokens_for_lm=8, lm_ratio=0.0, add_special_token_for_lm=False, restore_from=''),\n",
       " DataArguments(data_path=None, debug_data=False))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelArguments(), TrainingArguments(output_dir=\"./output\"), DataArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b430648d-69fa-4fd1-bd2b-b25e70b28b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ankner/gsm8k-CoT\")\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9fd4212-60d7-4b79-aba2-51cd6f84a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})\n",
    "eval_dataset = eval_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cd10f30-ae03-41d7-ac95-ef1d95fce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reasoning_trace(response):\n",
    "    \"\"\"\n",
    "    Extracts the reasoning trace from a response by splitting exactly at \"Therefore,\".\n",
    "\n",
    "    Args:\n",
    "        response (str): The full response including reasoning and the final answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted reasoning trace without the final answer.\n",
    "    \"\"\"\n",
    "    # Define the exact split phrase\n",
    "    split_phrase = \"Therefore,\"\n",
    "\n",
    "    # Search for the exact occurrence of \"Therefore,\"\n",
    "    match = response.find(split_phrase)\n",
    "\n",
    "    if match != -1:\n",
    "        reasoning_trace = response[:match].strip()  # Keep everything before \"Therefore,\"\n",
    "    else:\n",
    "        reasoning_trace = \"\"  # If not found, return the full response\n",
    "\n",
    "    return reasoning_trace\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    examples[\"reasoning_trace\"] = extract_reasoning_trace(examples[\"response\"])\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function)\n",
    "eval_dataset = eval_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aefa1588-c51b-48ef-adeb-5eecf2afd713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, i in enumerate(train_dataset):\n",
    "    if i['reasoning_trace'] == \"\":\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "713c0446-cd94-42b0-9528-c2721128c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(eval_dataset):\n",
    "    if i['reasoning_trace'] == \"\":\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df40f0aa-c0b3-409a-828a-47e9bd2d40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import pretrain_tokenize_function\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments()\n",
    "training_args = TrainingArguments(output_dir=\"./output\")\n",
    "data_args = DataArguments()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92291d9f-5671-44df-928d-9a0fadc9c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path = \"meta-llama/Llama-3.2-1B\"\n",
    "training_args.bf16 = True\n",
    "training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}  # manually add this argument in the code\n",
    "training_args.lm_ratio = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfdd6f1-845a-4b60-8027-8e4c4f2678cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b53d0d6-9f55-448e-aa21-375bf6cfe697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# import torch\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# icae = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", resume_download=True)\n",
    "# icae = icae.to(\"cuda\")\n",
    "# icae.config.vocab_size, icae.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e514bfdb-05e5-4d1a-bf8e-24eb0fcd7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13899776 || all params: 2485798912 || trainable%: 0.5591673539198975\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "from modeling_icae_multi_span import ICAE\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9264a71-c658-4c54-8dcb-33e81c7a76c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_args.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e725d9e-97a0-476c-b39f-14facd14270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the decoder...\n",
      "trainable params: 13899776 || all params: 2485798912 || trainable%: 0.5591673539198975\n",
      "Enabling gradient checkpointing...\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Initialize model and send it to CUDA device\n",
    "model = ICAE(model_args, training_args, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50a0f9e-183f-43f1-8167-ba7d6eca42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'response': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e2cb3d-afad-48b6-9fe8-8787e5ec0c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782e778fc4d140d8bcf8a36a23b1caa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a5c0e63d574295a6245c7b83b22507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ankner/gsm8k-CoT\")\n",
    "train_dataset = ds[\"train\"]\n",
    "eval_dataset = ds[\"test\"]\n",
    "train_dataset = train_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"}).shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.map(lambda example: {**example, \"text\": f\"{example['question']}\\n{example['response']}\"})\n",
    "\n",
    "memory_size = 128\n",
    "MEM_TOKENS = list(range(model.vocab_size, model.vocab_size + memory_size))\n",
    "\n",
    "train_dataset = train_dataset.map(pretrain_tokenize_function, batched=True, batch_size=1, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS, \"lm_ratio\": training_args.lm_ratio})\n",
    "eval_dataset = eval_dataset.map(pretrain_tokenize_function, batched=True, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS})   # don't add lm in the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd5c3058-b5d6-4551-8237-e26c9da36ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'response', 'text', 'input_ids', 'prompt_answer_ids', 'labels'],\n",
       "    num_rows: 7465\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c6016e-2862-46a1-bad0-1812741b390c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\",\n",
       " 'answer': '40',\n",
       " 'response': 'Each adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 × (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 × (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 × (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.\\n\\nTherefore, the final answer is 40.',\n",
       " 'text': \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\\nEach adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 × (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 × (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 × (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.\\n\\nTherefore, the final answer is 40.\",\n",
       " 'input_ids': [128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13],\n",
       " 'prompt_answer_ids': [128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128385,\n",
       "  128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13,\n",
       "  2],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13,\n",
       "  2]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b019ba5e-87c6-4e03-bb86-3ba7aaaa177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\",\n",
       " 'answer': '40',\n",
       " 'response': 'Each adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 × (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 × (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 × (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.\\n\\nTherefore, the final answer is 40.',\n",
       " 'text': \"Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?\\nEach adult has 32 teeth initially.\\n\\nFor the first person, 1/4 of 32 teeth were removed: 32 × (1/4) = 8 teeth removed.\\n\\nFor the second person, 3/8 of 32 teeth were removed: 32 × (3/8) = 12 teeth removed.\\n\\nFor the third person, 1/2 of 32 teeth were removed: 32 × (1/2) = 16 teeth removed.\\n\\nThe fourth person had exactly 4 teeth removed.\\n\\nAdding all teeth removed: 8 + 12 + 16 + 4 = 40 teeth.\\n\\nTherefore, the final answer is 40.\",\n",
       " 'input_ids': [128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13],\n",
       " 'prompt_answer_ids': [128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128257,\n",
       "  128385,\n",
       "  128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13,\n",
       "  2],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  128000,\n",
       "  28070,\n",
       "  12884,\n",
       "  449,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  50351,\n",
       "  369,\n",
       "  264,\n",
       "  1817,\n",
       "  455,\n",
       "  1306,\n",
       "  44114,\n",
       "  814,\n",
       "  1051,\n",
       "  3515,\n",
       "  15748,\n",
       "  26588,\n",
       "  6784,\n",
       "  13,\n",
       "  2435,\n",
       "  1051,\n",
       "  1766,\n",
       "  311,\n",
       "  617,\n",
       "  2204,\n",
       "  5219,\n",
       "  315,\n",
       "  20727,\n",
       "  18311,\n",
       "  11,\n",
       "  323,\n",
       "  1855,\n",
       "  1732,\n",
       "  1047,\n",
       "  1063,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  578,\n",
       "  1176,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  682,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  323,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  1047,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  1047,\n",
       "  4376,\n",
       "  315,\n",
       "  813,\n",
       "  18311,\n",
       "  7108,\n",
       "  11,\n",
       "  1418,\n",
       "  279,\n",
       "  1566,\n",
       "  1732,\n",
       "  1193,\n",
       "  1047,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  13,\n",
       "  3639,\n",
       "  596,\n",
       "  279,\n",
       "  2860,\n",
       "  1396,\n",
       "  315,\n",
       "  18311,\n",
       "  7108,\n",
       "  520,\n",
       "  279,\n",
       "  29106,\n",
       "  28913,\n",
       "  5380,\n",
       "  4959,\n",
       "  6822,\n",
       "  706,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  15453,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  1176,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  23,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  2132,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  18,\n",
       "  14,\n",
       "  23,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  717,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  2520,\n",
       "  279,\n",
       "  4948,\n",
       "  1732,\n",
       "  11,\n",
       "  220,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  315,\n",
       "  220,\n",
       "  843,\n",
       "  18311,\n",
       "  1051,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  843,\n",
       "  25800,\n",
       "  320,\n",
       "  16,\n",
       "  14,\n",
       "  17,\n",
       "  8,\n",
       "  284,\n",
       "  220,\n",
       "  845,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  791,\n",
       "  11999,\n",
       "  1732,\n",
       "  1047,\n",
       "  7041,\n",
       "  220,\n",
       "  19,\n",
       "  18311,\n",
       "  7108,\n",
       "  382,\n",
       "  33408,\n",
       "  682,\n",
       "  18311,\n",
       "  7108,\n",
       "  25,\n",
       "  220,\n",
       "  23,\n",
       "  489,\n",
       "  220,\n",
       "  717,\n",
       "  489,\n",
       "  220,\n",
       "  845,\n",
       "  489,\n",
       "  220,\n",
       "  19,\n",
       "  284,\n",
       "  220,\n",
       "  1272,\n",
       "  18311,\n",
       "  382,\n",
       "  55915,\n",
       "  11,\n",
       "  279,\n",
       "  1620,\n",
       "  4320,\n",
       "  374,\n",
       "  220,\n",
       "  1272,\n",
       "  13,\n",
       "  2]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.select([0])\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d15f088-6ab7-4858-99fa-d171294ddaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40290357-47d3-41b0-a0e2-bccc85b80846",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in train_dataset:\n",
    "    a.append((i['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a09b86-dbb8-47c0-9172-2ef758a955c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 28070,\n",
       " 12884,\n",
       " 449,\n",
       " 220,\n",
       " 843,\n",
       " 18311,\n",
       " 4024,\n",
       " 311,\n",
       " 279,\n",
       " 50351,\n",
       " 369,\n",
       " 264,\n",
       " 1817,\n",
       " 455,\n",
       " 1306,\n",
       " 44114,\n",
       " 814,\n",
       " 1051,\n",
       " 3515,\n",
       " 15748,\n",
       " 26588,\n",
       " 6784,\n",
       " 13,\n",
       " 2435,\n",
       " 1051,\n",
       " 1766,\n",
       " 311,\n",
       " 617,\n",
       " 2204,\n",
       " 5219,\n",
       " 315,\n",
       " 20727,\n",
       " 18311,\n",
       " 11,\n",
       " 323,\n",
       " 1855,\n",
       " 1732,\n",
       " 1047,\n",
       " 1063,\n",
       " 18311,\n",
       " 7108,\n",
       " 13,\n",
       " 578,\n",
       " 1176,\n",
       " 1732,\n",
       " 1047,\n",
       " 220,\n",
       " 16,\n",
       " 14,\n",
       " 19,\n",
       " 315,\n",
       " 682,\n",
       " 813,\n",
       " 18311,\n",
       " 7108,\n",
       " 11,\n",
       " 323,\n",
       " 279,\n",
       " 2132,\n",
       " 1732,\n",
       " 1047,\n",
       " 220,\n",
       " 18,\n",
       " 14,\n",
       " 23,\n",
       " 315,\n",
       " 813,\n",
       " 18311,\n",
       " 7108,\n",
       " 11,\n",
       " 279,\n",
       " 4948,\n",
       " 1732,\n",
       " 1047,\n",
       " 4376,\n",
       " 315,\n",
       " 813,\n",
       " 18311,\n",
       " 7108,\n",
       " 11,\n",
       " 1418,\n",
       " 279,\n",
       " 1566,\n",
       " 1732,\n",
       " 1193,\n",
       " 1047,\n",
       " 220,\n",
       " 19,\n",
       " 18311,\n",
       " 7108,\n",
       " 13,\n",
       " 3639,\n",
       " 596,\n",
       " 279,\n",
       " 2860,\n",
       " 1396,\n",
       " 315,\n",
       " 18311,\n",
       " 7108,\n",
       " 520,\n",
       " 279,\n",
       " 29106,\n",
       " 28913,\n",
       " 5380,\n",
       " 4959,\n",
       " 6822,\n",
       " 706,\n",
       " 220,\n",
       " 843,\n",
       " 18311,\n",
       " 15453,\n",
       " 382,\n",
       " 2520,\n",
       " 279,\n",
       " 1176,\n",
       " 1732,\n",
       " 11,\n",
       " 220,\n",
       " 16,\n",
       " 14,\n",
       " 19,\n",
       " 315,\n",
       " 220,\n",
       " 843,\n",
       " 18311,\n",
       " 1051,\n",
       " 7108,\n",
       " 25,\n",
       " 220,\n",
       " 843,\n",
       " 25800,\n",
       " 320,\n",
       " 16,\n",
       " 14,\n",
       " 19,\n",
       " 8,\n",
       " 284,\n",
       " 220,\n",
       " 23,\n",
       " 18311,\n",
       " 7108,\n",
       " 382,\n",
       " 2520,\n",
       " 279,\n",
       " 2132,\n",
       " 1732,\n",
       " 11,\n",
       " 220,\n",
       " 18,\n",
       " 14,\n",
       " 23,\n",
       " 315,\n",
       " 220,\n",
       " 843,\n",
       " 18311,\n",
       " 1051,\n",
       " 7108,\n",
       " 25,\n",
       " 220,\n",
       " 843,\n",
       " 25800,\n",
       " 320,\n",
       " 18,\n",
       " 14,\n",
       " 23,\n",
       " 8,\n",
       " 284,\n",
       " 220,\n",
       " 717,\n",
       " 18311,\n",
       " 7108,\n",
       " 382,\n",
       " 2520,\n",
       " 279,\n",
       " 4948,\n",
       " 1732,\n",
       " 11,\n",
       " 220,\n",
       " 16,\n",
       " 14,\n",
       " 17,\n",
       " 315,\n",
       " 220,\n",
       " 843,\n",
       " 18311,\n",
       " 1051,\n",
       " 7108,\n",
       " 25,\n",
       " 220,\n",
       " 843,\n",
       " 25800,\n",
       " 320,\n",
       " 16,\n",
       " 14,\n",
       " 17,\n",
       " 8,\n",
       " 284,\n",
       " 220,\n",
       " 845,\n",
       " 18311,\n",
       " 7108,\n",
       " 382,\n",
       " 791,\n",
       " 11999,\n",
       " 1732,\n",
       " 1047,\n",
       " 7041,\n",
       " 220,\n",
       " 19,\n",
       " 18311,\n",
       " 7108,\n",
       " 382,\n",
       " 33408,\n",
       " 682,\n",
       " 18311,\n",
       " 7108,\n",
       " 25,\n",
       " 220,\n",
       " 23,\n",
       " 489,\n",
       " 220,\n",
       " 717,\n",
       " 489,\n",
       " 220,\n",
       " 845,\n",
       " 489,\n",
       " 220,\n",
       " 19,\n",
       " 284,\n",
       " 220,\n",
       " 1272,\n",
       " 18311,\n",
       " 382,\n",
       " 55915,\n",
       " 11,\n",
       " 279,\n",
       " 1620,\n",
       " 4320,\n",
       " 374,\n",
       " 220,\n",
       " 1272,\n",
       " 13]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364105a-066c-4972-9f44-37cd6ff863ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
