Loaded from the checkpoint: None
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                | 0/20 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
 10%|███████▏                                                                | 2/20 [00:00<00:05,  3.24it/s]
{'loss': 1.5938, 'grad_norm': 0.06951949000358582, 'learning_rate': 4.75e-05, 'epoch': 1.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5938, 'grad_norm': 0.0691954642534256, 'learning_rate': 4.5e-05, 'epoch': 2.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5938, 'grad_norm': 0.06994561105966568, 'learning_rate': 4.25e-05, 'epoch': 3.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5938, 'grad_norm': 0.07003546506166458, 'learning_rate': 4e-05, 'epoch': 4.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5938, 'grad_norm': 0.07054409384727478, 'learning_rate': 3.7500000000000003e-05, 'epoch': 5.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5859, 'grad_norm': 0.07086922228336334, 'learning_rate': 3.5e-05, 'epoch': 6.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5859, 'grad_norm': 0.07184473425149918, 'learning_rate': 3.2500000000000004e-05, 'epoch': 7.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5859, 'grad_norm': 0.07214502990245819, 'learning_rate': 3e-05, 'epoch': 8.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5859, 'grad_norm': 0.07306177914142609, 'learning_rate': 2.7500000000000004e-05, 'epoch': 9.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5859, 'grad_norm': 0.07373038679361343, 'learning_rate': 2.5e-05, 'epoch': 10.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5781, 'grad_norm': 0.07414887845516205, 'learning_rate': 2.25e-05, 'epoch': 11.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5781, 'grad_norm': 0.07501998543739319, 'learning_rate': 2e-05, 'epoch': 12.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5781, 'grad_norm': 0.07554378360509872, 'learning_rate': 1.75e-05, 'epoch': 13.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07638049870729446, 'learning_rate': 1.5e-05, 'epoch': 14.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07609342783689499, 'learning_rate': 1.25e-05, 'epoch': 15.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07698425650596619, 'learning_rate': 1e-05, 'epoch': 16.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07758259773254395, 'learning_rate': 7.5e-06, 'epoch': 17.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07784567028284073, 'learning_rate': 5e-06, 'epoch': 18.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07795116305351257, 'learning_rate': 2.5e-06, 'epoch': 19.0}
input_ids shape:  torch.Size([1, 243])
prompt_answer_ids shape:  torch.Size([1, 133])
labels shape:  torch.Size([1, 133])
num_segments:  1
segment_length:  243
prompt_answer_embs shape:  torch.Size([1, 133, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 243
segment_input_ids shape:  torch.Size([1, 243])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 244])
mem_flag shape:  torch.Size([1, 244])
segment_input_embedding shape:  torch.Size([1, 244, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 244, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 133])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 133, 128256])
effective_logits shape:  torch.Size([132, 128256])
target_ids shape:  torch.Size([132])
{'loss': 1.5703, 'grad_norm': 0.07818378508090973, 'learning_rate': 0.0, 'epoch': 20.0}
{'train_runtime': 4.2758, 'train_samples_per_second': 4.678, 'train_steps_per_second': 4.678, 'train_loss': 1.58125, 'epoch': 20.0}
***** train metrics *****
  epoch                    =       20.0
  total_flos               =        0GF
  train_loss               =     1.5813
  train_runtime            = 0:00:04.27
  train_samples_per_second =      4.678
  train_steps_per_second   =      4.678
input_ids shape:  torch.Size([1, 131])
prompt_answer_ids shape:  torch.Size([1, 85])
labels shape:  torch.Size([1, 85])
num_segments:  1
segment_length:  131
prompt_answer_embs shape:  torch.Size([1, 85, 2048])
max_compressed_length:  1
compress_outputs shape:  torch.Size([1, 2048])
===============Segment 0=======================
start_idx: 0 | end_idx: 131
segment_input_ids shape:  torch.Size([1, 131])
append_sequence shape:  torch.Size([1, 1])
segment_input_ids shape after concat:  torch.Size([1, 132])
mem_flag shape:  torch.Size([1, 132])
segment_input_embedding shape:  torch.Size([1, 132, 2048])
Populated segment_input_embedding memory tokens
segment_compress_outputs (last hidden state) shape:  torch.Size([1, 132, 2048])
Filled in compressed memory for memory segment 0.
===============Segment 0 END=======================
decoder_mem_flag shape:  torch.Size([1, 85])
Populated decoder memory tokens with compressed outputs.
Populated decoder special memory tokens.
decoder_outputs logits shape:  torch.Size([1, 85, 128261])
effective_logits shape:  torch.Size([84, 128261])
target_ids shape:  torch.Size([84])
100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1412.22it/s]
***** eval metrics *****
  epoch                   =       20.0
  eval_loss               =      2.625
  eval_runtime            = 0:00:00.08
  eval_samples_per_second =     12.338
  eval_steps_per_second   =     12.338
Freezing the decoder...
trainable params: 13639680 || all params: 2485278720 || trainable%: 0.5488189268365039
Enabling gradient checkpointing...
Loading trained checkpoint from ./output
Running inference
100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.93s/it]
=========================== START ============================
Current line:  Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?
input_ids shape:  torch.Size([1, 105])
memory_slots shape:  torch.Size([1, 2048])
prompt_ids shape:  torch.Size([1, 1])
prompt_answer_embs shape:  torch.Size([1, 1, 2048])
decoder_input_embeddings shape:  torch.Size([1, 2, 2048])
output shape:  torch.Size([1, 2, 2048])
=========================== END ============================
Four adults with 32 teeth went to the dentist for a checkup after realizing they were having severe tooth pain. They were found to have different numbers of damaged teeth, and each person had some teeth removed. The first person had 1/4 of all his teeth removed, and the second person had 3/8 of his teeth removed, the third person had half of his teeth removed, while the last person only had 4 teeth removed. What's the total number of teeth removed at the dental clinic?
 laut [  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8
=========================================================================
